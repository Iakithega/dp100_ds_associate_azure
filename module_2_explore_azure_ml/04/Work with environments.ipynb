{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Work with environments\n",
        "\n",
        "\n",
        "When you run a script as an Azure Machine Learning job, you need to define the execution context for the job run. One key configuration is the compute target on which the script will be run. This could be the local workstation (in this case the compute instance), or a remote compute target such as the Azure Machine Learning managed compute cluster that is provisioned on-demand.\n",
        "\n",
        "In this notebook, you'll create a compute cluster and explore compute targets for jobs.\n",
        "\n",
        "## Before you start\n",
        "\n",
        "You'll need the latest version of the  **azure-ai-ml** package to run the code in this notebook. Run the cell below to verify that it is installed.\n",
        "\n",
        "> **Note**:\n",
        "> If the **azure-ai-ml** package is not installed, run `pip install azure-ai-ml` to install it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1665745893251
        }
      },
      "outputs": [],
      "source": [
        "pip show azure-ai-ml"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Connect to your workspace\n",
        "\n",
        "With the required SDK packages installed, now you're ready to connect to your workspace.\n",
        "\n",
        "To connect to a workspace, we need identifier parameters - a subscription ID, resource group name, and workspace name. Since you're working with a compute instance, managed by Azure Machine Learning, you can use the default values to connect to the workspace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azure.ai.ml import MLClient\n",
        "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
        "from dotenv import load_dotenv\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Overriding of current TracerProvider is not allowed\n",
            "Overriding of current LoggerProvider is not allowed\n",
            "Overriding of current MeterProvider is not allowed\n",
            "Attempting to instrument while already instrumented\n",
            "Attempting to instrument while already instrumented\n",
            "Attempting to instrument while already instrumented\n"
          ]
        }
      ],
      "source": [
        "load_dotenv()\n",
        "\n",
        "subscription_id = os.getenv(\"AZURE_SUBSCRIPTION_ID\")\n",
        "resource_group = os.getenv(\"AZURE_RESOURCE_GROUP\")\n",
        "workspace_name = os.getenv(\"AZURE_WORKSPACE_NAME\")\n",
        "\n",
        "\n",
        "\n",
        "try:\n",
        "    credential = DefaultAzureCredential()\n",
        "    # Check if given credential can get token successfully.\n",
        "    credential.get_token(\"https://management.azure.com/.default\")\n",
        "except Exception as ex:\n",
        "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
        "    credential = InteractiveBrowserCredential()\n",
        "\n",
        "\n",
        "ml_client = MLClient(\n",
        "    DefaultAzureCredential(), subscription_id, resource_group, workspace_name\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dvo-CPU-INS-DS11 : computeinstance\n",
            "basic-ci-danja : computeinstance\n",
            "Datastore: azureml_globaldatasets (Type: DatastoreType.AZURE_BLOB)\n",
            "Datastore: workspaceblobstore (Type: DatastoreType.AZURE_BLOB)\n",
            "Datastore: workspaceworkingdirectory (Type: DatastoreType.AZURE_FILE)\n",
            "Datastore: workspaceartifactstore (Type: DatastoreType.AZURE_BLOB)\n",
            "Datastore: workspacefilestore (Type: DatastoreType.AZURE_FILE)\n"
          ]
        }
      ],
      "source": [
        "# List available compute resources\n",
        "compute_list = ml_client.compute.list()\n",
        "for compute in compute_list:\n",
        "    print(compute.name, \":\", compute.type)\n",
        "\n",
        "\n",
        "# List all datastores in your workspace\n",
        "datastores = ml_client.datastores.list()\n",
        "for datastore in datastores:\n",
        "    print(f\"Datastore: {datastore.name} (Type: {datastore.type})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "public-docker-image-example-danja\n",
            "AzureML-ACPT-pytorch-1.13-py38-cuda11.7-gpu\n",
            "AzureML-TensorFlow2.4-Cuda11-OpenMpi4.1.0-py36\n",
            "AzureML-Scikit-learn0.24-Cuda11-OpenMpi4.1.0-py36\n",
            "AzureML-Pytorch1.7-Cuda11-OpenMpi4.1.0-py36\n"
          ]
        }
      ],
      "source": [
        "envs = ml_client.environments.list()\n",
        "for env in envs:\n",
        "    print(env.name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing src/conda_1-env.yml\n"
          ]
        }
      ],
      "source": [
        "%%writefile src/conda_1-env.yml\n",
        "name: basic-env-cpu\n",
        "channels:\n",
        "  - conda-forge\n",
        "dependencies:\n",
        "  - python=3.11\n",
        "  - scikit-learn\n",
        "  - pandas\n",
        "  - numpy\n",
        "  - matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Environment({'arm_type': 'environment_version', 'latest_version': None, 'image': 'pytorch/pytorch:latest', 'intellectual_property': None, 'is_anonymous': False, 'auto_increment_version': False, 'auto_delete_setting': None, 'name': 'public-docker-image-example-danja', 'description': 'Environment created from a public Docker image.', 'tags': {}, 'properties': {'azureml.labels': 'latest'}, 'print_as_yaml': False, 'id': '/subscriptions/6f83b388-7253-46ac-a2f7-300b7e3f313e/resourceGroups/pbi-dvo-rg/providers/Microsoft.MachineLearningServices/workspaces/dp100dvoML/environments/public-docker-image-example-danja/versions/1', 'Resource__source_path': '', 'base_path': 'c:\\\\Users\\\\dvolf\\\\python_projects\\\\azure_dp_100_2025\\\\dp100_ds_associate_azure\\\\module_2_explore_azure_ml\\\\04', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x000002025D757A40>, 'serialize': <msrest.serialization.Serializer object at 0x000002025B419130>, 'version': '1', 'conda_file': None, 'build': None, 'inference_config': None, 'os_type': 'Linux', 'conda_file_path': None, 'path': None, 'datastore': None, 'upload_hash': None, 'translated_conda_file': None})"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from azure.ai.ml.entities import Environment\n",
        "\n",
        "env_docker_image = Environment(\n",
        "    image=\"pytorch/pytorch:latest\",\n",
        "    name=\"public-docker-image-example-danja\",\n",
        "    description=\"Environment created from a public Docker image.\",\n",
        ")\n",
        "ml_client.environments.create_or_update(env_docker_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking for file at: C:\\Users\\dvolf\\python_projects\\azure_dp_100_2025\\dp100_ds_associate_azure\\module_2_explore_azure_ml\\04\\src\\conda_1-env.yml\n",
            "File exists: True\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Environment({'arm_type': 'environment_version', 'latest_version': None, 'image': 'mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04', 'intellectual_property': None, 'is_anonymous': False, 'auto_increment_version': False, 'auto_delete_setting': None, 'name': 'docker-image-plus-conda-example', 'description': 'Environment created from a Docker image plus Conda environment.', 'tags': {}, 'properties': {'azureml.labels': 'latest'}, 'print_as_yaml': False, 'id': '/subscriptions/6f83b388-7253-46ac-a2f7-300b7e3f313e/resourceGroups/pbi-dvo-rg/providers/Microsoft.MachineLearningServices/workspaces/dp100dvoML/environments/docker-image-plus-conda-example/versions/1', 'Resource__source_path': '', 'base_path': 'c:\\\\Users\\\\dvolf\\\\python_projects\\\\azure_dp_100_2025\\\\dp100_ds_associate_azure\\\\module_2_explore_azure_ml\\\\04', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x0000020262C486B0>, 'serialize': <msrest.serialization.Serializer object at 0x0000020262C4A690>, 'version': '1', 'conda_file': {'channels': ['conda-forge'], 'dependencies': ['python=3.11', 'scikit-learn', 'pandas', 'numpy', 'matplotlib'], 'name': 'basic-env-cpu'}, 'build': None, 'inference_config': None, 'os_type': 'Linux', 'conda_file_path': None, 'path': None, 'datastore': None, 'upload_hash': None, 'translated_conda_file': '{\\n  \"channels\": [\\n    \"conda-forge\"\\n  ],\\n  \"dependencies\": [\\n    \"python=3.11\",\\n    \"scikit-learn\",\\n    \"pandas\",\\n    \"numpy\",\\n    \"matplotlib\"\\n  ],\\n  \"name\": \"basic-env-cpu\"\\n}'})"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "file_path = Path(\"./src/conda_1-env.yml\").resolve()\n",
        "print(f\"Looking for file at: {file_path}\")\n",
        "print(f\"File exists: {file_path.exists()}\")\n",
        "\n",
        "\n",
        "\n",
        "env_docker_conda = Environment(\n",
        "    image=\"mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04\",\n",
        "    conda_file=file_path,\n",
        "    name=\"docker-image-plus-conda-example\",\n",
        "    description=\"Environment created from a Docker image plus Conda environment.\",\n",
        ")\n",
        "ml_client.environments.create_or_update(env_docker_conda)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "public-docker-image-example-danja\n",
            "AzureML-ACPT-pytorch-1.13-py38-cuda11.7-gpu\n",
            "AzureML-TensorFlow2.4-Cuda11-OpenMpi4.1.0-py36\n",
            "AzureML-Scikit-learn0.24-Cuda11-OpenMpi4.1.0-py36\n",
            "AzureML-Pytorch1.7-Cuda11-OpenMpi4.1.0-py36\n"
          ]
        }
      ],
      "source": [
        "envs = ml_client.environments.list()\n",
        "for env in envs:\n",
        "    print(env.name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1665745927409
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
        "from azure.ai.ml import MLClient\n",
        "\n",
        "try:\n",
        "    credential = DefaultAzureCredential()\n",
        "    # Check if given credential can get token successfully.\n",
        "    credential.get_token(\"https://management.azure.com/.default\")\n",
        "except Exception as ex:\n",
        "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
        "    credential = InteractiveBrowserCredential()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get a handle to workspace\n",
        "ml_client = MLClient.from_config(credential=credential)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run a script as a job\n",
        "\n",
        "To train a model, you'll first create the **diabetes_training.py** script in the **src** folder. The script uses the **diabetes.csv** file in the same folder as the training data.\n",
        "\n",
        "Note that you import libraries at the beginning of the script. Functions from these libraries are used to process the data and train the model. Whatever compute you use to run the script must have these libraries installed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile src/diabetes-training.py\n",
        "# import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import roc_curve\n",
        "\n",
        "# load the diabetes dataset\n",
        "print(\"Loading Data...\")\n",
        "diabetes = pd.read_csv('diabetes.csv')\n",
        "\n",
        "# separate features and labels\n",
        "X, y = diabetes[['Pregnancies','PlasmaGlucose','DiastolicBloodPressure','TricepsThickness','SerumInsulin','BMI','DiabetesPedigree','Age']].values, diabetes['Diabetic'].values\n",
        "\n",
        "# split data into training set and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\n",
        "\n",
        "# set regularization hyperparameter\n",
        "reg = 0.01\n",
        "\n",
        "# train a logistic regression model\n",
        "print('Training a logistic regression model with regularization rate of', reg)\n",
        "model = LogisticRegression(C=1/reg, solver=\"liblinear\").fit(X_train, y_train)\n",
        "\n",
        "# calculate accuracy\n",
        "y_hat = model.predict(X_test)\n",
        "acc = np.average(y_hat == y_test)\n",
        "print('Accuracy:', acc)\n",
        "\n",
        "# calculate AUC\n",
        "y_scores = model.predict_proba(X_test)\n",
        "auc = roc_auc_score(y_test,y_scores[:,1])\n",
        "print('AUC: ' + str(auc))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After you create the script, you can run the script as a job. The script uses common libraries. So you can use a curated environment that includes pandas, numpy, and scikit-learn, among others.\n",
        "\n",
        "The job uses the latest version of the curated environment: `AzureML-sklearn-0.24-ubuntu18.04-py37-cpu`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azure.ai.ml import command\n",
        "\n",
        "# configure job\n",
        "job = command(\n",
        "    code=\"./src\",\n",
        "    command=\"python diabetes-training.py\",\n",
        "    environment=\"AzureML-sklearn-0.24-ubuntu18.04-py37-cpu@latest\",\n",
        "    compute=\"aml-cluster\",\n",
        "    display_name=\"diabetes-train-curated-env\",\n",
        "    experiment_name=\"diabetes-training\"\n",
        ")\n",
        "\n",
        "# submit job\n",
        "returned_job = ml_client.create_or_update(job)\n",
        "aml_url = returned_job.studio_url\n",
        "print(\"Monitor your job at\", aml_url)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "While the job is running, you can already run the next cells."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## List environments\n",
        "\n",
        "Let's explore the environments within the workspace. \n",
        "\n",
        "In the previous job, you used one of the curated environments. To explore all environments that already exist in the workspace, you can list the environments: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "public-docker-image-example-danja\n",
            "AzureML-ACPT-pytorch-1.13-py38-cuda11.7-gpu\n",
            "AzureML-TensorFlow2.4-Cuda11-OpenMpi4.1.0-py36\n",
            "AzureML-Scikit-learn0.24-Cuda11-OpenMpi4.1.0-py36\n",
            "AzureML-Pytorch1.7-Cuda11-OpenMpi4.1.0-py36\n"
          ]
        }
      ],
      "source": [
        "envs = ml_client.environments.list()\n",
        "for env in envs:\n",
        "    print(env.name)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that all curated environments have names that begin **AzureML-** (you can't use this prefix for your own environments).\n",
        "\n",
        "To review a specific environment, you can retrieve an environment by its name and version. For example, you can retrieve the *description* and *tags* of the curated environment you used for the previous job:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "An environment for tasks such as regression, clustering, and classification with Scikit-learn. Contains the Azure ML SDK and additional python packages. {'Scikit-learn': '0.24.1', 'OS': 'Ubuntu18.04', 'Training': ''}\n"
          ]
        }
      ],
      "source": [
        "env = ml_client.environments.get(\"AzureML-sklearn-0.24-ubuntu18.04-py37-cpu\", version=44)\n",
        "print(env.description, env.tags)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create and use a custom environment\n",
        "\n",
        "If a curated environment doesn't include all the Python packages you need to run your script, you can create your own custom environment. By listing all necessary packages in an environment, you can easily re-run your scripts. All the dependencies are stored in the environment which you can then specify in the job configuration, independent of the compute you use.\n",
        "\n",
        "For example, you can create an environment simply from a Docker image. Certain frameworks like PyTorch will have a public Docker image that already includes everything you need. \n",
        "\n",
        "Let's create an environment from a Docker image:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Environment({'arm_type': 'environment_version', 'latest_version': None, 'image': 'mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04', 'intellectual_property': None, 'is_anonymous': False, 'auto_increment_version': False, 'auto_delete_setting': None, 'name': 'docker-image-example-danjaus', 'description': 'Environment created from a Docker image.', 'tags': {}, 'properties': {'azureml.labels': 'latest'}, 'print_as_yaml': False, 'id': '/subscriptions/6f83b388-7253-46ac-a2f7-300b7e3f313e/resourceGroups/pbi-dvo-rg/providers/Microsoft.MachineLearningServices/workspaces/dp100dvoML/environments/docker-image-example-danjaus/versions/1', 'Resource__source_path': '', 'base_path': 'c:\\\\Users\\\\dvolf\\\\python_projects\\\\azure_dp_100_2025\\\\dp100_ds_associate_azure\\\\module_2_explore_azure_ml\\\\04', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x00000202624FDA00>, 'serialize': <msrest.serialization.Serializer object at 0x0000020262464B30>, 'version': '1', 'conda_file': None, 'build': None, 'inference_config': None, 'os_type': 'Linux', 'conda_file_path': None, 'path': None, 'datastore': None, 'upload_hash': None, 'translated_conda_file': None})"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from azure.ai.ml.entities import Environment\n",
        "\n",
        "env_docker_image = Environment(\n",
        "    image=\"mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04\",\n",
        "    name=\"docker-image-example-danjaus\",\n",
        "    description=\"Environment created from a Docker image.\",\n",
        ")\n",
        "ml_client.environments.create_or_update(env_docker_image)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The environment is now registered in your workspace and you can reference it when you run a script as a job:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Class AutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
            "Class AutoDeleteConditionSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
            "Class BaseAutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
            "Class IntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
            "Class ProtectionLevelSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
            "Class BaseIntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
            "\u001b[32mUploading src (0.53 MBs): 100%|##########| 527753/527753 [00:00<00:00, 1124172.58it/s]\n",
            "\u001b[39m\n",
            "\n"
          ]
        },
        {
          "ename": "ResourceNotFoundError",
          "evalue": "(UserError) No environment exists for name: docker-image-example, version: 1, label: \nCode: UserError\nMessage: No environment exists for name: docker-image-example, version: 1, label: ",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mResourceNotFoundError\u001b[0m                     Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[26], line 14\u001b[0m\n\u001b[0;32m      4\u001b[0m job \u001b[38;5;241m=\u001b[39m command(\n\u001b[0;32m      5\u001b[0m     code\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./src\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      6\u001b[0m     command\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython diabetes-training.py\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m     experiment_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiabetes-training\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     11\u001b[0m )\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# submit job\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m returned_job \u001b[38;5;241m=\u001b[39m \u001b[43mml_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_or_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m aml_url \u001b[38;5;241m=\u001b[39m returned_job\u001b[38;5;241m.\u001b[39mstudio_url\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMonitor your job at\u001b[39m\u001b[38;5;124m\"\u001b[39m, aml_url)\n",
            "File \u001b[1;32mc:\\Users\\dvolf\\python_projects\\azure_dp_100_2025\\dp100_ds_associate_azure\\.venv\\Lib\\site-packages\\azure\\ai\\ml\\_ml_client.py:1256\u001b[0m, in \u001b[0;36mMLClient.create_or_update\u001b[1;34m(self, entity, **kwargs)\u001b[0m\n\u001b[0;32m   1240\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate_or_update\u001b[39m(\n\u001b[0;32m   1241\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1242\u001b[0m     entity: T,\n\u001b[0;32m   1243\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1244\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[0;32m   1245\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Creates or updates an Azure ML resource.\u001b[39;00m\n\u001b[0;32m   1246\u001b[0m \n\u001b[0;32m   1247\u001b[0m \u001b[38;5;124;03m    :param entity: The resource to create or update.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1253\u001b[0m \u001b[38;5;124;03m        , ~azure.ai.ml.entities.Environment, ~azure.ai.ml.entities.Component, ~azure.ai.ml.entities.Datastore]\u001b[39;00m\n\u001b[0;32m   1254\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1256\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_create_or_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mentity\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_operation_container\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_operations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\functools.py:909\u001b[0m, in \u001b[0;36msingledispatch.<locals>.wrapper\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    905\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[0;32m    906\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfuncname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires at least \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    907\u001b[0m                     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1 positional argument\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 909\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\dvolf\\python_projects\\azure_dp_100_2025\\dp100_ds_associate_azure\\.venv\\Lib\\site-packages\\azure\\ai\\ml\\_ml_client.py:1315\u001b[0m, in \u001b[0;36m_\u001b[1;34m(entity, operations, **kwargs)\u001b[0m\n\u001b[0;32m   1312\u001b[0m \u001b[38;5;129m@_create_or_update\u001b[39m\u001b[38;5;241m.\u001b[39mregister(Job)\n\u001b[0;32m   1313\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_\u001b[39m(entity: Job, operations, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1314\u001b[0m     module_logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating or updating job\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moperations\u001b[49m\u001b[43m[\u001b[49m\u001b[43mAzureMLResourceType\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mJOB\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_or_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mentity\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\dvolf\\python_projects\\azure_dp_100_2025\\dp100_ds_associate_azure\\.venv\\Lib\\site-packages\\azure\\core\\tracing\\decorator.py:116\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m func_tracing_attributes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    115\u001b[0m     span\u001b[38;5;241m.\u001b[39madd_attribute(key, value)\n\u001b[1;32m--> 116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\dvolf\\python_projects\\azure_dp_100_2025\\dp100_ds_associate_azure\\.venv\\Lib\\site-packages\\azure\\ai\\ml\\_telemetry\\activity.py:371\u001b[0m, in \u001b[0;36mmonitor_with_telemetry_mixin.<locals>.monitor.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    369\u001b[0m dimensions \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparameter_dimensions, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(custom_dimensions \u001b[38;5;129;01mor\u001b[39;00m {})}\n\u001b[0;32m    370\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m log_activity(logger, activity_name \u001b[38;5;129;01mor\u001b[39;00m f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, activity_type, dimensions) \u001b[38;5;28;01mas\u001b[39;00m activityLogger:\n\u001b[1;32m--> 371\u001b[0m     return_value \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    372\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parameter_dimensions:\n\u001b[0;32m    373\u001b[0m         \u001b[38;5;66;03m# collect from return if no dimensions from parameter\u001b[39;00m\n\u001b[0;32m    374\u001b[0m         activityLogger\u001b[38;5;241m.\u001b[39mactivity_info\u001b[38;5;241m.\u001b[39mupdate(_collect_from_return_value(return_value))\n",
            "File \u001b[1;32mc:\\Users\\dvolf\\python_projects\\azure_dp_100_2025\\dp100_ds_associate_azure\\.venv\\Lib\\site-packages\\azure\\ai\\ml\\operations\\_job_operations.py:695\u001b[0m, in \u001b[0;36mJobOperations.create_or_update\u001b[1;34m(self, job, description, compute, tags, experiment_name, skip_validation, **kwargs)\u001b[0m\n\u001b[0;32m    689\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (rest_job_resource\u001b[38;5;241m.\u001b[39mproperties\u001b[38;5;241m.\u001b[39mjob_type \u001b[38;5;241m==\u001b[39m RestJobType\u001b[38;5;241m.\u001b[39mPIPELINE) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m    690\u001b[0m     \u001b[38;5;28mhasattr\u001b[39m(rest_job_resource\u001b[38;5;241m.\u001b[39mproperties, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124midentity\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    691\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(rest_job_resource\u001b[38;5;241m.\u001b[39mproperties\u001b[38;5;241m.\u001b[39midentity, UserIdentity))\n\u001b[0;32m    692\u001b[0m ):\n\u001b[0;32m    693\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_headers_with_user_aml_token(kwargs)\n\u001b[1;32m--> 695\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_or_update_with_different_version_api\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrest_job_resource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrest_job_resource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    697\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_local_run(result):\n\u001b[0;32m    698\u001b[0m     ws_base_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_all_operations\u001b[38;5;241m.\u001b[39mall_operations[\n\u001b[0;32m    699\u001b[0m         AzureMLResourceType\u001b[38;5;241m.\u001b[39mWORKSPACE\n\u001b[0;32m    700\u001b[0m     ]\u001b[38;5;241m.\u001b[39m_operation\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39m_base_url\n",
            "File \u001b[1;32mc:\\Users\\dvolf\\python_projects\\azure_dp_100_2025\\dp100_ds_associate_azure\\.venv\\Lib\\site-packages\\azure\\ai\\ml\\operations\\_job_operations.py:741\u001b[0m, in \u001b[0;36mJobOperations._create_or_update_with_different_version_api\u001b[1;34m(self, rest_job_resource, **kwargs)\u001b[0m\n\u001b[0;32m    738\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rest_job_resource\u001b[38;5;241m.\u001b[39mproperties\u001b[38;5;241m.\u001b[39mjob_type \u001b[38;5;241m==\u001b[39m RestJobType\u001b[38;5;241m.\u001b[39mSWEEP:\n\u001b[0;32m    739\u001b[0m     service_client_operation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice_client_01_2024_preview\u001b[38;5;241m.\u001b[39mjobs\n\u001b[1;32m--> 741\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mservice_client_operation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_or_update\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    742\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrest_job_resource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresource_group_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_operation_scope\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresource_group_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    744\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkspace_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_workspace_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    745\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrest_job_resource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    747\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    749\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
            "File \u001b[1;32mc:\\Users\\dvolf\\python_projects\\azure_dp_100_2025\\dp100_ds_associate_azure\\.venv\\Lib\\site-packages\\azure\\core\\tracing\\decorator.py:116\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m func_tracing_attributes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    115\u001b[0m     span\u001b[38;5;241m.\u001b[39madd_attribute(key, value)\n\u001b[1;32m--> 116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\dvolf\\python_projects\\azure_dp_100_2025\\dp100_ds_associate_azure\\.venv\\Lib\\site-packages\\azure\\ai\\ml\\_restclient\\v2023_04_01_preview\\operations\\_jobs_operations.py:758\u001b[0m, in \u001b[0;36mJobsOperations.create_or_update\u001b[1;34m(self, resource_group_name, workspace_name, id, body, **kwargs)\u001b[0m\n\u001b[0;32m    755\u001b[0m response \u001b[38;5;241m=\u001b[39m pipeline_response\u001b[38;5;241m.\u001b[39mhttp_response\n\u001b[0;32m    757\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m200\u001b[39m, \u001b[38;5;241m201\u001b[39m]:\n\u001b[1;32m--> 758\u001b[0m     \u001b[43mmap_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatus_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    759\u001b[0m     error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deserialize\u001b[38;5;241m.\u001b[39mfailsafe_deserialize(_models\u001b[38;5;241m.\u001b[39mErrorResponse, pipeline_response)\n\u001b[0;32m    760\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HttpResponseError(response\u001b[38;5;241m=\u001b[39mresponse, model\u001b[38;5;241m=\u001b[39merror, error_format\u001b[38;5;241m=\u001b[39mARMErrorFormat)\n",
            "File \u001b[1;32mc:\\Users\\dvolf\\python_projects\\azure_dp_100_2025\\dp100_ds_associate_azure\\.venv\\Lib\\site-packages\\azure\\core\\exceptions.py:163\u001b[0m, in \u001b[0;36mmap_error\u001b[1;34m(status_code, response, error_map)\u001b[0m\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    162\u001b[0m error \u001b[38;5;241m=\u001b[39m error_type(response\u001b[38;5;241m=\u001b[39mresponse)\n\u001b[1;32m--> 163\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m error\n",
            "\u001b[1;31mResourceNotFoundError\u001b[0m: (UserError) No environment exists for name: docker-image-example, version: 1, label: \nCode: UserError\nMessage: No environment exists for name: docker-image-example, version: 1, label: "
          ]
        }
      ],
      "source": [
        "from azure.ai.ml import command\n",
        "\n",
        "# configure job\n",
        "job = command(\n",
        "    code=\"./src\",\n",
        "    command=\"python diabetes-training.py\",\n",
        "    environment=\"docker-image-example:1\",\n",
        "    compute=\"aml-cluster\",\n",
        "    display_name=\"diabetes-train-custom-env\",\n",
        "    experiment_name=\"diabetes-training\"\n",
        ")\n",
        "\n",
        "# submit job\n",
        "returned_job = ml_client.create_or_update(job)\n",
        "aml_url = returned_job.studio_url\n",
        "print(\"Monitor your job at\", aml_url)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p style=\"color:red;font-size:120%;background-color:yellow;font-weight:bold\"> The job will quickly fail! Review the error message. </p>\n",
        "\n",
        "The error message will tell you that there is no module named pandas. There are two possible causes for such an error:\n",
        "\n",
        "- The script uses pandas but didn't import the library (`import pandas as pd`). \n",
        "- The script does import the library at the top of the script but the compute didn't have the library installed (`pip install pandas`).\n",
        "\n",
        "After reviewing the `diabetes-training.py` script you can observe the script is correct, which means the library wasn't installed. In other words, the environment didn't include the necessary packages.\n",
        "\n",
        "Let's create a new environment, using the base Docker image used in the previous job. Now, you'll add a conda specification to ensure the necessary packages will be installed. First, run the following cell to create the conda specification file:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile src/conda-env.yml\n",
        "name: basic-env-cpu\n",
        "channels:\n",
        "  - conda-forge\n",
        "dependencies:\n",
        "  - python=3.11\n",
        "  - scikit-learn\n",
        "  - pandas\n",
        "  - numpy\n",
        "  - matplotlib"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that all necessary dependencies are included in the conda specification file for the script to run successfully.\n",
        "\n",
        "Create a new environment using the base Docker image **and** the conda specification file to add the necessary dependencies. Azure Machine Learning will build the conda environment on top of the Docker image you provided. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azure.ai.ml.entities import Environment\n",
        "\n",
        "env_docker_conda = Environment(\n",
        "    image=\"mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04\",\n",
        "    conda_file=\"./src/conda-env.yml\",\n",
        "    name=\"docker-image-plus-conda-example\",\n",
        "    description=\"Environment created from a Docker image plus Conda environment.\",\n",
        ")\n",
        "ml_client.environments.create_or_update(env_docker_conda)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, you can submit a job with the new environment to run the script:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azure.ai.ml import command\n",
        "\n",
        "# configure job\n",
        "job = command(\n",
        "    code=\"./src\",\n",
        "    command=\"python diabetes-training.py\",\n",
        "    environment=\"docker-image-plus-conda-example:1\",\n",
        "    compute=\"aml-cluster\",\n",
        "    display_name=\"diabetes-train-custom-env\",\n",
        "    experiment_name=\"diabetes-training\"\n",
        ")\n",
        "\n",
        "# submit job\n",
        "returned_job = ml_client.create_or_update(job)\n",
        "aml_url = returned_job.studio_url\n",
        "print(\"Monitor your job at\", aml_url)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Submitting the job with the new custom environment triggers the build of the environment. The first time you use a newly created environment, it can take 10-15 minutes to build the environment, which also means your job will take longer to complete. \n",
        "\n",
        "You can also choose to manually trigger the build of the environment before you submit a job. The environment only needs to be built the first time you use it. \n",
        "\n",
        "When the job triggers the build of a new environment, you can review the logs of the build in the **Outputs + logs** tab of the job. Open **azureml-logs/20_image_build_log.txt** to inspect the logs of the environment build. \n",
        "\n",
        "![Screenshot build logs](./images/screenshot-logs.png)"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
